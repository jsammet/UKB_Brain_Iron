nohup: ignoring input
/home/jsammet/env_swi/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:32: UserWarning: 
    There is an imbalance between your GPUs. You may want to exclude GPU 0 which
    has less than 75% of the memory or cores of GPU 1. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))
run.sh: line 5: t: command not found
Traceback (most recent call last):
  File "/home/jsammet/env_swi/lib/python3.10/site-packages/torchinfo/torchinfo.py", line 287, in forward_pass
    _ = model.to(device)(*x, **kwargs)
  File "/home/jsammet/env_swi/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1208, in _call_impl
    result = forward_call(*input, **kwargs)
  File "/home/jsammet/UKB_Brain_Iron/src/model.py", line 55, in forward
    x1 = self.initial(x)
  File "/home/jsammet/env_swi/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1208, in _call_impl
    result = forward_call(*input, **kwargs)
  File "/home/jsammet/UKB_Brain_Iron/src/model.py", line 28, in forward
    x = self.conv_block(x)
  File "/home/jsammet/env_swi/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1208, in _call_impl
    result = forward_call(*input, **kwargs)
  File "/home/jsammet/env_swi/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/jsammet/env_swi/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1208, in _call_impl
    result = forward_call(*input, **kwargs)
  File "/home/jsammet/env_swi/lib/python3.10/site-packages/torch/nn/modules/pooling.py", line 244, in forward
    return F.max_pool3d(input, self.kernel_size, self.stride,
  File "/home/jsammet/env_swi/lib/python3.10/site-packages/torch/_jit_internal.py", line 485, in fn
    return if_false(*args, **kwargs)
  File "/home/jsammet/env_swi/lib/python3.10/site-packages/torch/nn/functional.py", line 868, in _max_pool3d
    return torch.max_pool3d(input, kernel_size, stride, padding, dilation, ceil_mode)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 10.76 GiB total capacity; 438.62 MiB already allocated; 41.69 MiB free; 444.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/jsammet/UKB_Brain_Iron/main.py", line 96, in <module>
    main()
  File "/home/jsammet/UKB_Brain_Iron/main.py", line 75, in main
    print(summary(model, input_size=(1,1,256,288,48))) # batch size set to 1 instead params['batch_size']
  File "/home/jsammet/env_swi/lib/python3.10/site-packages/torchinfo/torchinfo.py", line 217, in summary
    summary_list = forward_pass(
  File "/home/jsammet/env_swi/lib/python3.10/site-packages/torchinfo/torchinfo.py", line 296, in forward_pass
    raise RuntimeError(
RuntimeError: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv3d: 3]
